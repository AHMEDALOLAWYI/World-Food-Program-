{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Masked RCNN\n",
    "Adapting the model from here https://github.com/multimodallearning/pytorch-mask-rcnn/blob/master/model.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision import *\n",
    "from fastai.vision.models.unet import _get_sfs_idxs, model_sizes, hook_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![FPN](https://cdn-images-1.medium.com/max/1600/1*D_EAjMnlR9v4LqHhEYZJLg.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequent usage of the words <b>downsampling</b> and <b>upsampling</b> paths refer to the bottom-up and top-down pathway respectively from the above diagram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LateralUpsampleMerge(nn.Module):\n",
    "    \"Merge the features coming from the downsample path (in `hook`) with the upsample path.\"\n",
    "    def __init__(self, ch, ch_lat, hook):\n",
    "        super().__init__()\n",
    "        self.hook = hook\n",
    "        self.Px_conv1 = conv2d(ch_lat, ch, ks=1, bias=True)\n",
    "#         self.Px_conv2 = conv2d(ch, cs, ks=3, padding=0, bias=True)\n",
    "            \n",
    "    def forward(self, x):\n",
    "        #Run a 1x1conv on the features from the downsampling path, upsample the output from P(x-1)\n",
    "        res = self.Px_conv1(self.hook.stored) + nn.functional.interpolate(x, scale_factor=2)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note to self\n",
    "Intend to explore whether or not stride 2 conv is better than maxpooling (P5 - P6)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPN(nn.Module):\n",
    "    \"\"\"\n",
    "    Creates upsampling path by hooking activations from the downsampling path. Tested on ResNet50.\n",
    "    \n",
    "    INPUT:\n",
    "        encoder: Default ResNet50\n",
    "        chs: Number of intermediate channels to use between convolutions\n",
    "    \n",
    "    RETURNS:\n",
    "        [p2,p3,p4,p5,p6]: [Tensor]\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder:nn.Module, chs:int) -> [Tensor]:\n",
    "    \n",
    "        super().__init__()\n",
    "        \n",
    "        #This runs dummy data through the encoder to get the right channel numbers after each layer C1 through C5\n",
    "        imsize = (256,256)\n",
    "        sfs_szs = model_sizes(encoder, size=imsize)\n",
    "        sfs_idxs = list(reversed(_get_sfs_idxs(sfs_szs)))\n",
    "        \n",
    "        #Attaching hooks to the relevant layers C2 to C5 so we can get their activations during the\n",
    "        #upsampling path\n",
    "        self.encoder = encoder\n",
    "        \n",
    "        #The link between C5 and P5\n",
    "        #TODO: will a stride 2 conv be better?\n",
    "        self.c5_p5 = nn.Sequential(\n",
    "            conv2d(sfs_szs[-1][1], chs, ks=1, bias=True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "#         self.p5_conv2 = nn.Sequential(\n",
    "#             nn.ZeroPad2d(1),\n",
    "#             conv2d(chs,chs,ks=3,padding=0,bias=True)\n",
    "#         )\n",
    "        self.p5_conv2 = conv2d(chs,chs,ks=3,bias=True)\n",
    "        \n",
    "        #Link between P5 and P6\n",
    "        self.p5_p6 = nn.MaxPool2d(kernel_size=1, stride=2)\n",
    "        \n",
    "        #These are the idxs of C4, C3, and C2 respectively\n",
    "        idx  = list(reversed(sfs_idxs[-2:-5:-1]))\n",
    "        self.sfs = hook_outputs([encoder[i] for i in idx])\n",
    "        \n",
    "        #This handles the mapping from P5 -> P4 -> P3 -> P2\n",
    "        self.merges = nn.ModuleList([LateralUpsampleMerge(chs, sfs_szs[idx][1], hook) \n",
    "                                     for idx,hook in zip(idx, self.sfs)])\n",
    "        \n",
    "        #One final conv to smoothen things out after the merge\n",
    "        self.final_convs = [conv2d(chs, chs, ks=3, stride=1, bias=True) for _ in idx+[1]]\n",
    "           \n",
    "    def forward(self, x):\n",
    "        c5 = self.encoder(x)\n",
    "        p_states = [self.c5_p5(c5.clone())]\n",
    "        #Mapping P5 through P2 one by one\n",
    "        for merge in self.merges: p_states = [merge(p_states[0])] + p_states\n",
    "         \n",
    "        #Extra convs after the lateral upsampling\n",
    "        for i, conv in enumerate(self.final_convs):\n",
    "            p_states[i] = conv(p_states[i])\n",
    "            \n",
    "        #Doing P6 at the end\n",
    "        p6 = self.p5_p6(p_states[-1])\n",
    "        p_states += [p6]\n",
    "        return p_states\n",
    "    \n",
    "    def __del__(self):\n",
    "        if hasattr(self, \"sfs\"): self.sfs.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(10,3,448,448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpn = FPN(create_body(models.resnet50,pretrained=False),256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpnres = fpn(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(112, 112), (56, 56), (28, 28), (14, 14), (7, 7)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shapes = [fpnres[0].shape,fpnres[1].shape,fpnres[2].shape,fpnres[3].shape,fpnres[4].shape]\n",
    "shapes = [(s[-2],s[-1]) for s in shapes]\n",
    "shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Find stride of model at each stage\n",
    "dims = [s[-1] for s in shapes]\n",
    "stride = [int(x.shape[-1]/d) for d in dims]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 8, 16, 32, 64]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37632"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "3*112*112"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anchor Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Length of square anchor side in pixels, we might need to play with this\n",
    "RPN_ANCHOR_SCALES = (32, 64, 128, 256,350)\n",
    "\n",
    "# Ratios of anchors at each cell (width/height)\n",
    "# A value of 1 represents a square anchor, and 0.5 is a wide anchor\n",
    "RPN_ANCHOR_RATIOS = [0.5, 1, 2]\n",
    "\n",
    "# Anchor Stride\n",
    "# Set to 1 so that anchor is given to every grid on the feature map\n",
    "RPN_ANCHOR_STRIDE = 1\n",
    "\n",
    "# The strides of each layer of the FPN Pyramid as calculated above.\n",
    "BACKBONE_STRIDES = [4, 8, 16, 32, 64]\n",
    "\n",
    "#Shape of each feature map as calculated above\n",
    "BACKBONE_SHAPES = shapes\n",
    "\n",
    "#To test things out\n",
    "scales = RPN_ANCHOR_SCALES[0]\n",
    "feature_stride = BACKBONE_STRIDES[0]\n",
    "anchor_stride = RPN_ANCHOR_STRIDE\n",
    "shape =BACKBONE_SHAPES[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all combinations of scales and ratios\n",
    "scales, ratios = np.meshgrid(np.array(scales), np.array(RPN_ANCHOR_RATIOS))\n",
    "scales = scales.flatten()\n",
    "ratios = ratios.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([32, 32, 32])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5, 1. , 2. ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "heights = scales / np.sqrt(ratios)\n",
    "widths = scales * np.sqrt(ratios)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems to cover the entire area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x1a2ab44cc0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "#We have covered every single possible point on a 256*256 IMG\n",
    "plt.scatter(shifts_x,shifts_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_anchors(scales, ratios, shape, feature_stride, anchor_stride):\n",
    "    \"\"\"\n",
    "    scales: 1D array of anchor sizes in pixels. Example: [32, 64, 128]\n",
    "    ratios: 1D array of anchor ratios of width/height. Example: [0.5, 1, 2]\n",
    "    shape: [height, width] spatial shape of the feature map over which\n",
    "            to generate anchors.\n",
    "    feature_stride: Stride of the feature map relative to the image in pixels.\n",
    "    anchor_stride: Stride of anchors on the feature map. For example, if the\n",
    "        value is 2 then generate anchors for every other feature map pixel.\n",
    "    \"\"\"\n",
    "    # Get all combinations of scales and ratios\n",
    "    scales, ratios = np.meshgrid(np.array(scales), np.array(ratios))\n",
    "    scales = scales.flatten()\n",
    "    ratios = ratios.flatten()\n",
    "\n",
    "    # Enumerate heights and widths from scales and ratios\n",
    "    heights = scales / np.sqrt(ratios)\n",
    "    widths = scales * np.sqrt(ratios)\n",
    "\n",
    "    # Enumerate shifts in feature space\n",
    "    shifts_y = np.arange(0, shape[0], anchor_stride) * feature_stride\n",
    "    shifts_x = np.arange(0, shape[1], anchor_stride) * feature_stride\n",
    "    shifts_x, shifts_y = np.meshgrid(shifts_x, shifts_y)\n",
    "\n",
    "    # Enumerate combinations of shifts, widths, and heights\n",
    "    box_widths, box_centers_x = np.meshgrid(widths, shifts_x)\n",
    "    box_heights, box_centers_y = np.meshgrid(heights, shifts_y)\n",
    "\n",
    "    # Reshape to get a list of (y, x) and a list of (h, w)\n",
    "    box_centers = np.stack(\n",
    "        [box_centers_y, box_centers_x], axis=2).reshape([-1, 2])\n",
    "    box_sizes = np.stack([box_heights, box_widths], axis=2).reshape([-1, 2])\n",
    "\n",
    "    # Convert to corner coordinates (y1, x1, y2, x2)\n",
    "    boxes = np.concatenate([box_centers - 0.5 * box_sizes,\n",
    "                            box_centers + 0.5 * box_sizes], axis=1)\n",
    "    return boxes\n",
    "\n",
    "def generate_pyramid_anchors(scales, ratios, feature_shapes, feature_strides,\n",
    "                             anchor_stride):\n",
    "    \"\"\"Generate anchors at different levels of a feature pyramid. Each scale\n",
    "    is associated with a level of the pyramid, but each ratio is used in\n",
    "    all levels of the pyramid.\n",
    "\n",
    "    Returns:\n",
    "    anchors: (Num anchors, 4). All generated anchors in one array. Sorted\n",
    "        with the same order of the given scales. So, anchors of scale[0] come\n",
    "        first, then anchors of scale[1], and so on.\n",
    "    \"\"\"\n",
    "    # Anchors\n",
    "    # [anchor_count, (y1, x1, y2, x2)]\n",
    "    anchors = []\n",
    "    for i in range(len(scales)):\n",
    "        anchors.append(generate_anchors(scales[i], ratios, feature_shapes[i],\n",
    "                                        feature_strides[i], anchor_stride))\n",
    "    return np.concatenate(anchors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = generate_pyramid_anchors(RPN_ANCHOR_SCALES,RPN_ANCHOR_RATIOS,BACKBONE_SHAPES,BACKBONE_STRIDES,RPN_ANCHOR_STRIDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50127, 4)"
      ]
     },
     "execution_count": 475,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ignoring Anchors That Are Out of Bounds\n",
    "Might want to implement that since the Faster-RCNN paper suggests the following:\n",
    "\n",
    "The anchor boxes that cross image boundaries need to be handled with care. During training, we ignore all cross-boundary anchors so they do not contribute to the loss. For a typical 1000 x 600 image, there will be roughly 20000 (60 x 40 x 9) anchors in total. With the cross-boundary anchors ignored, there are about 6000 anchors per image for training. If the boundary-crossing outliers are not ignored in training, they introduce large, difficult to correct error terms in the objective, and training does not converge. During testing, however, we still apply the fully convolutional RPN to the entire image. This may generate cross- boundary proposal boxes, which we clip to the image boundary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from random import shuffle\n",
    "idx = np.random.randint(0,len(anchors),50)\n",
    "sample_boxes = anchors[49750:49850]\n",
    "\n",
    "# sample_boxes  = sample_boxes.clip(min=0,max=448)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111, aspect='equal')\n",
    "for b in sample_boxes:\n",
    "    #Since matplotlib expects something to be between 0 and 1\n",
    "    y1, x1, y2, x2 = b/448\n",
    "    if y1 < 0 or x1 <0 : continue\n",
    "    if y2 > 1 or x2 > 1: continue\n",
    "    x = x2-x1;\n",
    "    y=y2-y1\n",
    "    ax.add_patch(\n",
    "         patches.Rectangle(\n",
    "            (x1,y1),x,y,\n",
    "            fill=False      # remove background\n",
    "         ) )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region Proposal Network\n",
    "\n",
    "#### Lack of dropout\n",
    "Just noticed the lack of dropout in this model. Might be worth checking into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RPN(nn.Module):\n",
    "    \"\"\"Builds the model of Region Proposal Network.\n",
    "    \n",
    "    Inputs:\n",
    "        anchors_per_location: number of anchors per pixel in the feature map\n",
    "        anchor_stride: Controls the density of anchors. Typically 1 (anchors for\n",
    "                       every pixel in the feature map), or 2 (every other pixel).\n",
    "\n",
    "    Returns:\n",
    "        rpn_logits: [batch, H, W, 2] Anchor classifier logits (before softmax)\n",
    "        rpn_probs: [batch, W, W, 2] Anchor classifier probabilities.\n",
    "        rpn_bbox: [batch, H, W, (dy, dx, log(dh), log(dw))] Deltas to be\n",
    "                  applied to anchors.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, ratios_per_anchor=3, anchor_stride=1, ch_in=256):\n",
    "        super(RPN, self).__init__()\n",
    "        self.ratios_per_anchor = ratios_per_anchor\n",
    "        self.anchor_stride = anchor_stride\n",
    "        self.ch_in = ch_in\n",
    "        #This conv has a 'same' padding so that grid size does not change\n",
    "        self.conv_shared = conv2d(ch_in,512,ks=3,stride=anchor_stride,bias=True)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        #Since we will have anchors of various ratios at each position on the featuregrid, \n",
    "        #we need that many predictions\n",
    "        #Class predicts whether or not an anchor has a match\n",
    "        #BBox predicts the corresponding coordinates\n",
    "        self.conv_class = conv2d(512,2 * ratios_per_anchor,ks=1,stride=anchor_stride,bias=True)\n",
    "        self.softmax = nn.Softmax(dim=2)\n",
    "        self.conv_bbox = nn.Conv2d(512, 4 * ratios_per_anchor, kernel_size=1, stride=1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "#         import pdb; pdb.set_trace()\n",
    "        x = self.relu(self.conv_shared(x))\n",
    "        \n",
    "        ###Classification - Absence/Presence of Match\n",
    "        # (bs,2*numratios,h,w)\n",
    "        class_logits = self.conv_class(x)\n",
    "        \n",
    "        # bs, h, w, 2*ratios\n",
    "        class_logits = class_logits.permute(0,2,3,1)\n",
    "        class_logits = class_logits.contiguous()\n",
    "        # (bs, num anchors, 2)\n",
    "        class_logits = class_logits.view(x.size()[0], -1, 2)\n",
    "        \n",
    "        ###Bounding Box Regressor\n",
    "        bbox = self.conv_bbox(x)\n",
    "        # (bs, h, w, 4*ratios)\n",
    "        bbox = bbox.permute(0,2,3,1)\n",
    "        bbox = bbox.contiguous()\n",
    "        # (bs, num anchors, 4)\n",
    "        bbox = bbox.view(x.size()[0], -1, 4)\n",
    "                \n",
    "        return [class_logits, bbox]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identifying regions of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rpn = RPN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through pyramid layers\n",
    "layer_outputs = []  # list of lists\n",
    "for p in fpnres:\n",
    "    layer_outputs.append(rpn(p))\n",
    "\n",
    "outputs = list(zip(*layer_outputs))\n",
    "outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "rpn_class_logits, rpn_bbox = outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Near Maximum Suppression (NMS)\n",
    "This removes bounding boxes that have high overlap with each other (prioritises boxes that the model is more confident in)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hard_nms(bboxes, scores, topn=2000, iou_thresh=0.7):\n",
    "    # Perhaps top 2000 for training and 1000 for inference\n",
    "    indices = scores.argsort()\n",
    "    bboxes = bboxes[indices]\n",
    "    final_results = []\n",
    "\n",
    "    res = bboxes\n",
    "\n",
    "    while len(res)!=0:\n",
    "        box = res[-1]\n",
    "        final_results.append(box)\n",
    "        if len(final_results) == topn:\n",
    "            return torch.stack(final_results,0)\n",
    "        res = res[:-1]\n",
    "\n",
    "        start_max = np.maximum(res[:, 0:2], box[0:2])\n",
    "        end_min = np.minimum(res[:, 2:4], box[2:4])\n",
    "        lengths = end_min - start_max\n",
    "        intersec_map = lengths[:, 0] * lengths[:, 1]\n",
    "        intersec_map[np.logical_or(lengths[:, 0] < 0, lengths[:, 1] < 0)] = 0\n",
    "        iou_map_cur = intersec_map / ((res[:, 2] - res[:, 0]) * (res[:, 3] - res[:, 1]) + (box[2] - box[0]) * (\n",
    "            box[3] - box[1]) - intersec_map +1e-16)\n",
    "        res = res[iou_map_cur < iou_thresh]\n",
    "\n",
    "    return np.array(cdd_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some helper functions to calculate IoU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(inputs, targets):\n",
    "    \"\"\"    \n",
    "    Compute the sizes of the intersections of `inputs` with `targets`.\n",
    "    Inputs:\n",
    "        inputs,targets in (y1,x1,y2,x2) form\n",
    "    \"\"\"\n",
    "    i, t = inputs.size(0), targets.size(0)\n",
    "    inputs, targets = inputs.unsqueeze(1).expand(i,t,4), targets.unsqueeze(0).expand(i,t,4)\n",
    "    bottom_left = torch.max(inputs[...,:2], targets[...,:2])\n",
    "    top_right = torch.min(inputs[...,2:], targets[...,2:])\n",
    "    sizes = torch.clamp(top_right - bottom_left, min=0) \n",
    "    return sizes[...,0] * sizes[...,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def IoU_values(inputs, targets):\n",
    "    \"Compute the IoU values of `inputs` with `targets`.\"\n",
    "    inter = intersection(inputs, targets)\n",
    "    input_sz = (inputs[:,2] - inputs[:,0]) * (inputs[:,3] - inputs[:,1])\n",
    "    target_sz = (targets[:,2] - targets[:,0]) * (targets[:,3] - targets[:,1])\n",
    "    union = input_sz.unsqueeze(1) + target_sz.unsqueeze(0) - inter\n",
    "    iou = inter/(union+1e-8)\n",
    "    #This way we filter out boxes that have zero areas\n",
    "#     iou[input_sz == 0] = 1.\n",
    "    return iou"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#this is faster than above\n",
    "def nms(b, s, topn=2000, thresh=0.75):\n",
    "    \"\"\"\n",
    "    Inputs:\n",
    "        b: tensor of boxes (numbox,4)\n",
    "        s: tensor of scores (numbox,)\n",
    "        thres: threshold of NMS\n",
    "    \"\"\"\n",
    "    idx_sort = s.argsort(descending=True)\n",
    "    boxes, scores = b[idx_sort], s[idx_sort]\n",
    "    to_keep, indexes = [], torch.LongTensor(range_of(scores))\n",
    "    while len(scores) > 0 :\n",
    "        to_keep.append(idx_sort[indexes[0]])\n",
    "        cdd = boxes[0]\n",
    "        boxes, scores, indexes  = boxes[1:],scores[1:], indexes[1:]\n",
    "        if len(boxes)==0:\n",
    "            break\n",
    "        iou_vals = IoU_values(boxes, cdd.unsqueeze(0)).squeeze()\n",
    "        mask_keep = iou_vals < thresh\n",
    "        if mask_keep.nonzero().shape[0] == 0: break\n",
    "        boxes, scores, indexes = boxes[mask_keep], scores[mask_keep], indexes[mask_keep]\n",
    "    return to_keep[:topn]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made this more legible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def preds2bbox(deltas,boxes):\n",
    "#     \"\"\"Applies the given deltas to the given boxes.\n",
    "#     boxes: [N, 4] where each row is y1, x1, y2, x2\n",
    "#     deltas: [N, 4] where each row is [dy, dx, log(dh), log(dw)]\n",
    "#     \"\"\"\n",
    "# #     import pdb; pdb.set_trace()\n",
    "#     # Convert to y, x, h, w\n",
    "#     height = tensor(boxes[:, 2] - boxes[:, 0]).float()\n",
    "#     width = tensor(boxes[:, 3] - boxes[:, 1]).float()\n",
    "#     center_y = tensor(boxes[:, 0]).float() + 0.5 * height\n",
    "#     center_x = tensor(boxes[:, 1]).float() + 0.5 * width\n",
    "#     # Apply deltas\n",
    "#     center_y += deltas[:, 0] * height\n",
    "#     center_x += deltas[:, 1] * width\n",
    "#     height *= torch.exp(deltas[:, 2])\n",
    "#     width *= torch.exp(deltas[:, 3])\n",
    "#     # Convert back to y1, x1, y2, x2\n",
    "#     y1 = center_y - 0.5 * height\n",
    "#     x1 = center_x - 0.5 * width\n",
    "#     y2 = y1 + height\n",
    "#     x2 = x1 + width\n",
    "#     result = torch.stack([y1, x1, y2, x2], dim=1)\n",
    "#     return result\n",
    "\n",
    "def preds2bbox(deltas,boxes):\n",
    "    \"\"\"Applies the given deltas to the given boxes.\n",
    "    boxes: [N, 4] where each row is y1, x1, y2, x2\n",
    "    deltas: [N, 4] where each row is [dy, dx, log(dh), log(dw)]\n",
    "    \"\"\"\n",
    "    hw = boxes[...,2:]-boxes[...,:2]\n",
    "    center_yx = (boxes[...,2:]+boxes[...,:2])*0.5\n",
    "    hw_ = hw.clone() * deltas[...,:2]\n",
    "    center_yx += hw_\n",
    "    hw *= torch.exp(deltas[...,2:])\n",
    "    y1x1 = center_yx - 0.5 * hw\n",
    "    y2x2 = center_yx + 0.5 * hw\n",
    "    result = torch.cat([y1x1,y2x2],-1)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Needs work!\n",
    "We can't parallelise this, so we process each item in the batch one-by-one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_1_proposal(inputs, anchors, imgsize=(448,448),nms_threshold=0.7, max_proposals=2000):\n",
    "    \"\"\"\n",
    "    Handles one proposal only.\n",
    "    Receives anchor offets and confidence scores and selects candidates to move on to the next stage. \n",
    "    Filtering is done based on anchor scores and\n",
    "    non-max suppression to remove overlaps. \n",
    "\n",
    "    Inputs:\n",
    "        rpn_probs: [batch, anchors, (bg prob, fg prob)]\n",
    "        rpn_bbox: [batch, anchors, (dy, dx, log(dh), log(dw))]\n",
    "\n",
    "    Returns:\n",
    "        Proposals in normalized coordinates [batch, rois, (y1, x1, y2, x2)]\n",
    "    \"\"\"\n",
    "    # Remove batch dimension\n",
    "    inputs[0] = inputs[0].squeeze(0)\n",
    "    inputs[1] = inputs[1].squeeze(0)\n",
    "\n",
    "    # Box Scores. Use the foreground class confidence. [Batch, num_rois, 1]\n",
    "    scores = F.softmax(inputs[0],-1)[:, 1]\n",
    "\n",
    "    # Box Offsets [num_anchors, 4]\n",
    "    offsets = inputs[1]\n",
    "    RPN_BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])\n",
    "    std_dev = tensor(RPN_BBOX_STD_DEV)[None,:]\n",
    "    if torch.cuda.is_available():\n",
    "        std_dev = std_dev.cuda()\n",
    "    offsets = offsets * std_dev.float()\n",
    "\n",
    "    # Improve performance by trimming to top anchors by score\n",
    "    # and doing the rest on the smaller subset.\n",
    "    pre_nms_limit = min(6000, anchors.shape[0])\n",
    "    scores, order = scores.sort(descending=True)\n",
    "    order = order[:pre_nms_limit]\n",
    "    scores = scores[:pre_nms_limit]\n",
    "    offsets = offsets[order.data, :] \n",
    "    anchors = tensor(anchors[order.data, :]).float()\n",
    "    \n",
    "    # Apply deltas to anchors to get refined anchors.\n",
    "    # [num anchors, (y1, x1, y2, x2)]\n",
    "    boxes = preds2bbox(offsets,anchors)\n",
    "\n",
    "    # Clip to image boundaries. [num anchors, (y1, x1, y2, x2)]\n",
    "    height, width = imgsize\n",
    "    # ok we make the assumption that the images are square\n",
    "    boxes = boxes.clamp(min=0,max=height)\n",
    "    #\n",
    "    keep_idx = nms(boxes, scores, topn=max_proposals, thresh=nms_threshold)\n",
    "\n",
    "    return normalize_boxes(boxes[keep_idx], imgsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to loop through to convert activations to BB coords and run NMS.\n",
    "This step is a little slow, needs improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "bs = 10"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# boxes = []\n",
    "# %time for i in range(bs): boxes += [process_1_proposal([rpn_class_logits[i],rpn_bbox[i]],anchors,448)]\n",
    "# boxes = torch.stack(boxes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalise the boxes so that they are between 0 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "height,width = 448,448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_boxes(boxes, img_sz=(448,448)):\n",
    "    height,width = img_sz\n",
    "    norm = tensor([height, width, height, width]).float()\n",
    "    res = boxes / norm\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# %time normalize_boxes(boxes).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Training Targets\n",
    "We get 2000 proposals after applying NMS on RPN proposals, we need to match them to ground truth targets.\n",
    "\n",
    "The paper recommends only training on a certain amount of ROIs only, i.e. those that pass the threshold. It also recommends to have a pos:neg ratio of 1:3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_ROIS_PER_IMAGE = 200\n",
    "ROI_POSITIVE_RATIO = 0.33\n",
    "#We will perturb the targets slightly for regularisation purposes\n",
    "BBOX_STD_DEV = np.array([0.1, 0.1, 0.2, 0.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "b2 = tensor([[2,180,200,300],[300,25,350,50]]).float()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This works for batchsize one only."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_targets(proposals, gt_class_ids, gt_boxes):\n",
    "    \"\"\"\n",
    "    Matches proposed ROIs with Ground Truth BBoxes and Labels based on IoU.\n",
    "    \"\"\"\n",
    "    # Currently only supports batchsize 1\n",
    "    proposals = proposals.squeeze(0)\n",
    "    gt_class_ids = gt_class_ids.squeeze(0).int()\n",
    "    gt_boxes = gt_boxes.squeeze(0)\n",
    "    \n",
    "    #Find IoU values between proposed boxes and target boxes\n",
    "    overlaps = IoU_values(proposals,gt_boxes)\n",
    "    #Find out which target box matches each proposed box best.\n",
    "    roi_iou_max = torch.max(overlaps, dim=1)[0]\n",
    "    \n",
    "    # 1. Positive ROIs are those with >= 0.5 IoU with a targ box\n",
    "    positive_roi_bool = roi_iou_max >= 0.5\n",
    "\n",
    "    if positive_roi_bool.sum()>0:\n",
    "        positive_indices = torch.nonzero(positive_roi_bool)[:, 0]\n",
    "\n",
    "        positive_count = int(TRAIN_ROIS_PER_IMAGE *\n",
    "                             ROI_POSITIVE_RATIO)\n",
    "        rand_idx = torch.randperm(positive_indices.size()[0])\n",
    "        rand_idx = rand_idx[:positive_count]\n",
    "        if torch.cuda.is_available():\n",
    "            rand_idx = rand_idx.cuda()\n",
    "\n",
    "        positive_indices = positive_indices[rand_idx]\n",
    "        positive_count = positive_indices.size()[0]\n",
    "        positive_rois = proposals[positive_indices.data,:]\n",
    "\n",
    "        positive_overlaps = overlaps[positive_indices.data,:]\n",
    "        roi_gt_box_assignment = torch.max(positive_overlaps, dim=1)[1]\n",
    "        roi_gt_boxes = gt_boxes[roi_gt_box_assignment.data,:]\n",
    "        roi_gt_class_ids = gt_class_ids[roi_gt_box_assignment.data]\n",
    "\n",
    "        offsets = find_offsets(positive_rois.data, roi_gt_boxes.data)\n",
    "        std_dev = torch.from_numpy(BBOX_STD_DEV).float()\n",
    "        if torch.cuda.is_available():\n",
    "            std_dev = std_dev.cuda()\n",
    "        offsets /= std_dev\n",
    "    else:\n",
    "        # if we don't find any matches\n",
    "        positive_count = 0\n",
    "        \n",
    "    # 2. Negative ROIs are those with < 0.5 with every GT box. Skip crowds.\n",
    "    negative_roi_bool = roi_iou_max < 0.5\n",
    "    if negative_roi_bool.sum()>0 and positive_count>0:\n",
    "        #Sample negative examples in a fixed ratio\n",
    "        negative_indices = torch.nonzero(negative_roi_bool)[:, 0]\n",
    "        r = 1.0 / ROI_POSITIVE_RATIO\n",
    "        negative_count = int(r * positive_count - positive_count)\n",
    "        rand_idx = torch.randperm(negative_indices.size()[0])\n",
    "        rand_idx = rand_idx[:negative_count]\n",
    "        if torch.cuda.is_available():\n",
    "            rand_idx = rand_idx.cuda()\n",
    "        negative_indices = negative_indices[rand_idx]\n",
    "        negative_count = negative_indices.size()[0]\n",
    "        negative_rois = proposals[negative_indices.data, :]\n",
    "    else:\n",
    "        negative_count = 0\n",
    "            \n",
    "    #PADDING\n",
    "    # Append negative ROIs and pad box offsets with zeroes since they are disregarded during training.\n",
    "    if positive_count > 0 and negative_count > 0:\n",
    "        rois = torch.cat((positive_rois, negative_rois), dim=0)\n",
    "        zeros = torch.zeros(negative_count).int()\n",
    "        if torch.cuda.is_available():\n",
    "            zeros= zeros.cuda()\n",
    "        roi_gt_class_ids = torch.cat([roi_gt_class_ids, zeros], dim=0)\n",
    "        zeros = torch.zeros(negative_count,4)\n",
    "        if torch.cuda.is_available():\n",
    "            zeros= zeros.cuda()\n",
    "        offsets = torch.cat([offsets, zeros], dim=0)\n",
    "\n",
    "    elif positive_count > 0:\n",
    "        rois = positive_rois\n",
    "    elif negative_count > 0:\n",
    "        rois = negative_rois\n",
    "        zeros = torch.zeros(negative_count).int()\n",
    "        if torch.cuda.is_available():\n",
    "            zeros= zeros.cuda()\n",
    "        roi_gt_class_ids = zeros\n",
    "        zeros = torch.zeros(negative_count,4)\n",
    "        if torch.cuda.is_available():\n",
    "            zeros= zeros.cuda()\n",
    "        offsets = zeros\n",
    "    else:\n",
    "        rois =torch.FloatTensor()\n",
    "        roi_gt_class_ids = torch.IntTensor()\n",
    "        offsets = torch.FloatTensor()\n",
    "        if torch.cuda.is_available():\n",
    "            rois = rois.cuda()\n",
    "            roi_gt_class_ids = roi_gt_class_ids.cuda()\n",
    "            offsets = deltas.cuda()\n",
    "    return rois, roi_gt_class_ids, offsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_offsets(box, gt_box):\n",
    "    \"\"\"Calculate offsets needed to transform proposed ROI to Ground Truth Bounding Box.\n",
    "    box and gt_box are [N, (y1, x1, y2, x2)]\n",
    "    \n",
    "    Returns:\n",
    "     (nbox,4)\n",
    "    \"\"\"\n",
    "\n",
    "    height = box[:, 2] - box[:, 0]\n",
    "    width = box[:, 3] - box[:, 1]\n",
    "    center_y = (box[:, 0] + 0.5 * height).float()\n",
    "    center_x = (box[:, 1] + 0.5 * width).float()\n",
    "\n",
    "    gt_height = gt_box[:, 2] - gt_box[:, 0]\n",
    "    gt_width = gt_box[:, 3] - gt_box[:, 1]\n",
    "    gt_center_y = gt_box[:, 0] + 0.5 * gt_height\n",
    "    gt_center_x = gt_box[:, 1] + 0.5 * gt_width\n",
    "\n",
    "    dy = (gt_center_y - center_y) / height.float()\n",
    "    dx = (gt_center_x - center_x) / width.float()\n",
    "    dh = torch.log(gt_height / height.float())\n",
    "    dw = torch.log(gt_width / width.float())\n",
    "\n",
    "    result = torch.stack([dy, dx, dh, dw], dim=1)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "rois, target_class_ids, target_deltas = create_training_targets(tensor(anchors).float(),tensor([[1,2]]),b2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrcnn_feature_maps = fpnres[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "POOL_SIZE = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [rois/448] + [m[0,:] for m in mrcnn_feature_maps]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log2(x):\n",
    "    \"\"\"Implementatin of Log2. Pytorch doesn't have a native implemenation.\"\"\"\n",
    "    ln2 = torch.log(torch.FloatTensor([2.0]))\n",
    "    if x.is_cuda:\n",
    "        ln2 = ln2.cuda()\n",
    "    return torch.log(x) / ln2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roi_align.crop_and_resize import CropAndResize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from roi_align.crop_and_resize import CropAndResizeFunction\n",
    "#Install from https://github.com/longcw/RoIAlign.pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool_size = 7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've mentioned before (or did we) that each layer of the feature pyramid is responsible for detecting images of different sizes. Now that we have our RoIs, we can map them to the layer of the pyramid that they are supposed to be responsible for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formally, we assign an RoI of width w and height h (on the input image to the network) to the level P_k of our feature pyramid by:\n",
    "\n",
    "Section 4.2, Equation 1 from https://arxiv.org/pdf/1612.03144.pdf\n",
    "\n",
    "Analogous to the ResNet-based Faster R-CNN system that uses C4 as the single-scale feature map, we set k0 to 4"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pyramid_roi_align(inputs, image_shape:tuple,pool_size:int=7):\n",
    "    \"\"\"Implements ROI Pooling on multiple levels of the feature pyramid.\n",
    "\n",
    "    Params:\n",
    "    - pool_size: [height, width] of the output pooled regions. Usually [7, 7]\n",
    "    - image_shape: [height, width, channels]. Shape of input image in pixels\n",
    "\n",
    "    Inputs:\n",
    "    - boxes: [batch, num_boxes, (y1, x1, y2, x2)] in normalized\n",
    "             coordinates.\n",
    "    - Feature maps: List of feature maps from different levels of the pyramid.\n",
    "                    Each is [batch, channels, height, width]\n",
    "\n",
    "    Output:\n",
    "    Pooled regions in the shape: [num_boxes, height, width, channels].\n",
    "    The width and height are those specific in the pool_shape in the layer\n",
    "    constructor.\n",
    "    \"\"\"\n",
    "\n",
    "    # Currently only supports batchsize 1\n",
    "    for i in range(len(inputs)):\n",
    "        inputs[i] = inputs[i].squeeze(0)\n",
    "\n",
    "    # Crop boxes [batch, num_boxes, (y1, x1, y2, x2)] in normalized coords\n",
    "    boxes = inputs[0]\n",
    "\n",
    "    # Feature Maps. List of feature maps from different level of the\n",
    "    # feature pyramid. Each is [batch, height, width, channels]\n",
    "    feature_maps = inputs[1:]\n",
    "\n",
    "    # Assign each ROI to a level in the pyramid based on the ROI area.\n",
    "    y1, x1, y2, x2 = boxes.chunk(4, dim=1)\n",
    "    h = y2 - y1\n",
    "    w = x2 - x1\n",
    "\n",
    "    # Equation 1 in the Feature Pyramid Networks paper. Account for\n",
    "    # the fact that our coordinates are normalized here.\n",
    "    # e.g. a 224x224 ROI (in pixels) maps to P4\n",
    "    image_area = torch.FloatTensor([float(image_shape[0]*image_shape[1])])\n",
    "    if boxes.is_cuda:\n",
    "        image_area = image_area.cuda()\n",
    "    roi_level = 4 + log2(torch.sqrt(h*w)/(224.0/torch.sqrt(image_area)))\n",
    "    roi_level = roi_level.round().int()\n",
    "    roi_level = roi_level.clamp(2,5)\n",
    "\n",
    "    cropandresize = CropAndResize(7,7)\n",
    "    # Loop through levels and apply ROI pooling to each. P2 to P5.  \n",
    "    pooled = []\n",
    "    box_to_level = []\n",
    "    for i, level in enumerate(range(2, 6)):\n",
    "        ix  = roi_level==level\n",
    "        if not ix.any():\n",
    "            continue\n",
    "        ix = torch.nonzero(ix)[:,0]\n",
    "        level_boxes = boxes[ix.data, :]\n",
    "\n",
    "        # Keep track of which box is mapped to which level\n",
    "        box_to_level.append(ix.data)\n",
    "\n",
    "        # Stop gradient propogation to ROI proposals, this step will \n",
    "        level_boxes = level_boxes.detach()\n",
    "\n",
    "        # Crop and Resize\n",
    "        # We crop each feature map output from each level in the pyramid based on the RoIs that are assigned to them\n",
    "        ind = torch.zeros(level_boxes.size()[0]).int()\n",
    "        if level_boxes.is_cuda:\n",
    "            ind = ind.cuda()\n",
    "        feature_maps[i] = feature_maps[i].unsqueeze(0)  #CropAndResizeFunction needs batch dimension\n",
    "        pooled_features = cropandresize(feature_maps[i], level_boxes, ind)\n",
    "        pooled.append(pooled_features)\n",
    "\n",
    "    # Pack pooled features into one tensor\n",
    "    pooled = torch.cat(pooled, dim=0)\n",
    "\n",
    "    # Pack box_to_level mapping into one array and add another\n",
    "    # column representing the order of pooled boxes\n",
    "    box_to_level = torch.cat(box_to_level, dim=0)\n",
    "\n",
    "    # Rearrange pooled features to match the order of the original boxes\n",
    "    _, box_to_level = torch.sort(box_to_level)\n",
    "    pooled = pooled[box_to_level, :, :]\n",
    "\n",
    "    return pooled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = pyramid_roi_align(inputs,(448,448))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 256, 7, 7])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the layer that handles classification and regression of each box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_bn_relu(ni, nf, ks):\n",
    "    return nn.Sequential(conv2d(ni, nf, ks=ks, stride=1,padding=0,bias=True),\n",
    "                         nn.BatchNorm2d(nf,eps=0.001, momentum=0.01), nn.ReLU(inplace=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BBoxClassifierRegressor(nn.Module):\n",
    "    def __init__(self, depth=256, pool_size:int=7, image_shape=(448,448), num_classes=12):\n",
    "        super().__init__()\n",
    "        self.depth = depth\n",
    "        self.pool_size = pool_size\n",
    "        self.image_shape = image_shape if isinstance(image_shape,tuple) else (image_shape,image_shape)\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        self.convs = nn.Sequential(conv_bn_relu(self.depth,1024,ks=self.pool_size),conv_bn_relu(1024,1024,ks=1))\n",
    "        self.bbox_clas = nn.Linear(1024, num_classes)\n",
    "        #HMM, can't we just do 4 instead of 4*classes\n",
    "        self.bbox_reg = nn.Linear(1024, num_classes * 4)\n",
    "    def forward(self, mrcnn_feature_maps , rois):\n",
    "        x = pyramid_roi_align([rois]+mrcnn_feature_maps,self.image_shape, self.pool_size )\n",
    "        x = self.convs(x)\n",
    "        x = x.view(-1,1024)\n",
    "        \n",
    "        mrcnn_class_logits = self.bbox_clas(x)\n",
    "\n",
    "        mrcnn_bbox = self.bbox_reg(x)\n",
    "        mrcnn_bbox = mrcnn_bbox.view(mrcnn_bbox.size()[0], -1, 4)\n",
    "        return [mrcnn_class_logits, mrcnn_bbox]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "bbc = BBoxClassifierRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = bbc([m[0,:] for m in mrcnn_feature_maps],rois/448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 12])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([178, 12, 4])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskRCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    encoder : Tested on Resnet50 head. \n",
    "    pretrained : If we use a model from 'models', we can opt for a pretrained model\n",
    "    chs: Number of intermediate dimensions\n",
    "    num_classes: Number of output classes\n",
    "    img_sz: Input image dimensions\n",
    "    \"\"\"\n",
    "    def __init__(self, encoder=None, pretrained=True, chs=256, num_classes=20, img_sz = (448,448)):\n",
    "        super().__init__()\n",
    "        if not encoder:\n",
    "            encoder = create_body(models.resnet50,pretrained=pretrained)\n",
    "        \n",
    "        self.img_sz = img_sz\n",
    "            \n",
    "        #Feature Pyramid Network\n",
    "        self.fpn = FPN(encoder,chs)  \n",
    "        \n",
    "        #Create Anchor Boxes\n",
    "        self.anchors = generate_pyramid_anchors(RPN_ANCHOR_SCALES,RPN_ANCHOR_RATIOS,BACKBONE_SHAPES,BACKBONE_STRIDES,RPN_ANCHOR_STRIDE)\n",
    "#TODO: Turn this into PyTorch Tensors to use the GPU\n",
    "#         if torch.cuda.is_available():\n",
    "#             self.anchors = self.anchors.cuda()\n",
    "\n",
    "        #Region Proposal Network\n",
    "        self.rpn = RPN(ratios_per_anchor=len(RPN_ANCHOR_RATIOS),ch_in=chs)\n",
    "        \n",
    "        # BBox Classifier and Regressor\n",
    "        self.bbox_clas_reg = BBoxClassifierRegressor(depth=chs, num_classes=num_classes)\n",
    "        \n",
    "    def forward(self,inp, targ=None):\n",
    "        \"\"\"\n",
    "        A forward pass through the model should output the all predicted bounding boxes with their respective predicted classes.\n",
    "        \n",
    "        Target BBox should be normalised between 0 and 1\n",
    "        \"\"\"\n",
    "        bs = inp.shape[0]\n",
    "        if targ:\n",
    "            gt_class_ids, gt_boxes = targ\n",
    "        #Feed input through FPN\n",
    "        #Returns : [p2_out, p3_out, p4_out, p5_out, p6_out]\n",
    "        feature_maps = self.fpn(inp)\n",
    "        #P6 not used in MRCNN layers\n",
    "        mrcnn_feature_maps = feature_maps[:-1]\n",
    "        \n",
    "        #Loop through each feature map and feed it through the RPN\n",
    "        layer_outputs = []  # list of lists\n",
    "        for p in feature_maps:\n",
    "            layer_outputs.append(self.rpn(p))\n",
    "        \n",
    "        # Concatenate layer outputs\n",
    "        # e.g. [[logits1, bbox1], [logits2, bbox2]] => [[logits1,logits2], [bbox1, bbox2]]\n",
    "        outputs = list(zip(*layer_outputs))\n",
    "        outputs = [torch.cat(list(o), dim=1) for o in outputs]\n",
    "        # We get logits that suggest whether or not an object is present, and the offsets to get the proposed region from the anchors\n",
    "        rpn_class_logits, rpn_bbox = outputs\n",
    "        \n",
    "        \n",
    "        #Proposal processing can't be done in a batch\n",
    "        boxes = []\n",
    "        for i in range(bs): boxes += [process_1_proposal([rpn_class_logits[i], rpn_bbox[i]], self.anchors, self.img_sz,nms_threshold=0.7, max_proposals=2000)]\n",
    "        boxes = torch.stack(boxes) \n",
    "\n",
    "        if self.training:\n",
    "            print('in training mode')\n",
    "            #To save computation time, since we only sample some boxes to contribute to the loss, we do it here before running\n",
    "            #subsequent computations.\n",
    "            rois, target_class_ids,target_offsets = create_training_targets(boxes, gt_class_ids, gt_boxes)\n",
    "\n",
    "            #If we get relevant detections\n",
    "            if len(rois) > 0:\n",
    "                mrcnn_class_logits, mrcnn_bbox = self.bbox_clas_reg(mrcnn_feature_maps,rois)\n",
    "            else:\n",
    "                mrcnn_class_logits = torch.FloatTensor()\n",
    "                mrcnn_class = torch.IntTensor()\n",
    "                mrcnn_bbox = torch.FloatTensor()\n",
    "        \n",
    "            return [rpn_class_logits, rpn_bbox, target_class_ids, mrcnn_class_logits, target_offsets, mrcnn_bbox]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn(1,3,448,448)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrcnn = MaskRCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in training mode\n"
     ]
    }
   ],
   "source": [
    "res = mrcnn(x,(tensor([[1,2]]),b2/448))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss Functions\n",
    "Now that we have a model that can make predictions. We need to define the loss functions (and a few targets). \n",
    "First, let's deal with the region proposals from the RPN. We predict a ton of anchors, but we only want a subset of them to be responsible for giving us a reasonable prediction.\n",
    "\n",
    "More specifically, we assign the following:\n",
    "1. If an anchor overlaps a GT box with IoU >= 0.7 then it's positive.\n",
    "2. If an anchor overlaps a GT box with IoU < 0.3 then it's negative.\n",
    "3. If an anchor is neither of the above, it is neutral and we ignore it.\n",
    "\n",
    "Aside from that, we also want all GT boxes to be matched with something (even if it doesn't fulfill the IoU ratio). So at the end if there remains any GT box that hasn't been assigned to any anchor, we assign it to the closest anchor box."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt_class_ids, gt_boxes = tensor([[1,2]]),b2/448"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchs = tensor(mrcnn.anchors/448).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparameter that controls how many non-neutral boxes we have (neutral boxes get disregarded in the loss)\n",
    "RPN_TRAIN_ANCHORS_PER_IMAGE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_rpn_targets(anchors, gt_class_ids, gt_boxes):\n",
    "    iou  = IoU_values(anchors,gt_boxes)\n",
    "    rpn_match = torch.zeros(anchors.shape[0],dtype=torch.int32)\n",
    "\n",
    "    #if the best IoU with all GT boxes are less than .3, its a negative\n",
    "    values1, idx1  = iou.max(1)\n",
    "    msk = torch.nonzero(values1<0.3)\n",
    "    rpn_match[msk] = -1\n",
    "    #if the best IoU is greater than .7, its a positive\n",
    "    msk = torch.nonzero(values1>0.7)\n",
    "    rpn_match[msk] = 1      \n",
    "    #For each GT box, the anchor that has the highest overlap with it will be a positive no matter what\n",
    "    values2, idx2  = iou.max(0)\n",
    "    rpn_match[idx2] = 1\n",
    "    \n",
    "    #We don't want an imbalanced training regime\n",
    "    #Check if positive exceeds the preset hyperparameter\n",
    "    pos_ids = np.where(rpn_match == 1)[0]\n",
    "    extra = len(pos_ids) - (RPN_TRAIN_ANCHORS_PER_IMAGE // 2)\n",
    "    \n",
    "    if extra > 0:\n",
    "        # Reset the extra ones to neutral so they don't count\n",
    "        ids = np.random.choice(ids, extra, replace=False)\n",
    "        rpn_match[ids] = 0\n",
    "        \n",
    "    #Repeat for negatives\n",
    "    neg_ids = np.where(rpn_match == -1)[0]\n",
    "    extra = len(neg_ids) - (RPN_TRAIN_ANCHORS_PER_IMAGE -\n",
    "                        torch.sum(rpn_match == 1).item())\n",
    "    if extra > 0:\n",
    "        # Rest the extra ones to neutral\n",
    "        ids = np.random.choice(neg_ids, extra, replace=False)\n",
    "        rpn_match[ids] = 0\n",
    "    #Check again since we might have removed some positive examples    \n",
    "    updated_pos_ids = np.where(rpn_match == 1)[0]\n",
    "    \n",
    "    #Negatives and neutrals with have zeroes as their bb targets\n",
    "    rpn_bbox = torch.zeros((RPN_TRAIN_ANCHORS_PER_IMAGE, 4)).float()\n",
    "    ix = 0 \n",
    "    anchs,gts=[],[]\n",
    "    for i, a in zip(updated_pos_ids, anchors[updated_pos_ids]):\n",
    "        # idx1 contains the index of the GT box that fits that the ith anchor best\n",
    "        gt = gt_boxes[idx1[i]]\n",
    "        # Requires the batch dimension\n",
    "        gt = gt.unsqueeze(0)\n",
    "        # Find the offsets required to transform anchor to ground truth bb\n",
    "        rpn_bbox[ix] = find_offsets(tensor(a).unsqueeze(0),gt)\n",
    "    #     rpn_bbox[ix] /= RPN_BBOX_STD_DEV\n",
    "        ix += 1\n",
    "    return rpn_match, rpn_bbox"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
